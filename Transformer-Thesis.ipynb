{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import time \n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 55\n",
    "NUM_FEATURES = 512\n",
    "IMG_SIZE = 128\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [video_name, tag]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [video_name, tag]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [video_name, tag]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"augmented-videos\"\n",
    "train=pd.DataFrame(columns=['video_name','tag'])\n",
    "test=pd.DataFrame(columns=['video_name','tag'])\n",
    "\n",
    "print(train)\n",
    "j = 0\n",
    "for root, files, videos in os.walk(dataset_path):\n",
    "\n",
    "    class_name = root.split(\"/\")\n",
    "#     if j>=5:\n",
    "#         break\n",
    "    if len(class_name)>1:\n",
    "        j += 1\n",
    "        thresh = math.ceil(len(videos)*0.8)\n",
    "        random.shuffle(videos)\n",
    "        print(\"root---\",root,videos)\n",
    "        \n",
    "        for i,  video in enumerate(videos) :\n",
    "            if i <= thresh:\n",
    "                train=train.append({\"video_name\":os.path.join(class_name[-1],video),\"tag\":class_name[-1]},ignore_index = True)\n",
    "            else:\n",
    "                test=test.append({\"video_name\":os.path.join(class_name[-1],video),\"tag\":class_name[-1]},ignore_index = True)\n",
    "#     for video in videos[thresh:]:\n",
    "            \n",
    "   \n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "train.to_csv('train_fold.csv')   \n",
    "test.to_csv('test_fold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Croping center of the frame\n",
    "\"\"\"\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 3) - (min_dim // 5) \n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    crop= frame[start_y : start_y + min_dim, start_x : start_x + min_dim] # croping center of the frame\n",
    "    \n",
    "    return cv2.resize(crop,(128,128)) # resizing image to 128,128 pixel\n",
    "\n",
    "# Following method is modified from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def load_video(path, max_frames=0):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = frame[:, :, [2, 1, 0]] # accepting blue channel only\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set an empty vocabulary. Received: vocabulary=[]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m build_feature_extractor()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Label preprocessing with StringLookup.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m label_processor \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringLookup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_oov_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_processor\u001b[38;5;241m.\u001b[39mget_vocabulary())\n",
      "File \u001b[1;32m~\\Desktop\\thesis\\myworld\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\string_lookup.py:320\u001b[0m, in \u001b[0;36mStringLookup.__init__\u001b[1;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, idf_weights, invert, output_mode, pad_to_max_tokens, sparse, encoding, name, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sparse=True` can only be used with the \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow backend.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m encoding\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_oov_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_oov_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43moov_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moov_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43midf_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midf_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_max_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_max_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocabulary_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_non_tensor_positional_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\thesis\\myworld\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:248\u001b[0m, in \u001b[0;36mIndexLookup.__init__\u001b[1;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary_dtype, vocabulary, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, name, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_weights_const \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_weights\u001b[38;5;241m.\u001b[39mvalue()\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocabulary \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midf_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# When restoring from a keras SavedModel, the loading code will\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# expect to find and restore a lookup_table attribute on the layer.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# This table needs to be uninitialized as a StaticHashTable cannot\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# be initialized twice.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uninitialized_lookup_table()\n",
      "File \u001b[1;32m~\\Desktop\\thesis\\myworld\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:421\u001b[0m, in \u001b[0;36mIndexLookup.set_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    418\u001b[0m     idf_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(idf_weights)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocabulary\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot set an empty vocabulary. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: vocabulary=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocabulary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    426\u001b[0m oov_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oov_start_index()\n\u001b[0;32m    427\u001b[0m token_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_start_index()\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set an empty vocabulary. Received: vocabulary=[]"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.VGG16(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.vgg16.preprocess_input\n",
    "    \n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n",
    "\n",
    "\n",
    "# Label preprocessing with StringLookup.\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train[\"class_label\"]), mask_token=None\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Video Feature extraction \n",
    "\"\"\"\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_features` are what we will feed to our sequence model.\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "\n",
    "        # Pad shorter videos.\n",
    "        if len(frames) < MAX_SEQ_LENGTH:\n",
    "            diff = MAX_SEQ_LENGTH - len(frames)\n",
    "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "            frames = np.concatenate((frames, padding))\n",
    "\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholder to store the features of the current video.\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                if np.mean(batch[j, :]) > 0.0:\n",
    "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                        batch[None, j, :]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    temp_frame_features[i, j, :] = 0.0\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "\n",
    "    return frame_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"position_embeddings\": self.position_embeddings,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.5\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "            \n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"attention\": self.attention\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    This model architecture for Dense model\n",
    "    Dense model give 1024 features\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(1024)\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "adm = Adam(learning_rate=0.001) \n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 8\n",
    "    num_heads = 4\n",
    "    # classes = len(label_processor.get_vocabulary())\n",
    "    classes=30\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None))\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"models/ckpt/\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    model = get_compiled_model()\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        validation_split=0.10,\n",
    "        epochs=20, \n",
    "        callbacks=[checkpoint],\n",
    "        batch_size= 64\n",
    "    )\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    _, accuracy = model.evaluate(test_data, test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "training_time = time.time()\n",
    "trained_model, history = run_experiment()\n",
    "end_time = time.time()-training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    # Pad shorter videos.\n",
    "    if len(frames) < MAX_SEQ_LENGTH:\n",
    "        diff = MAX_SEQ_LENGTH - len(frames)\n",
    "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "        frames = np.concatenate((frames, padding))\n",
    "\n",
    "    frames = frames[None, ...]\n",
    "\n",
    "    # Extract features from the frames of the current video.\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            if np.mean(batch[j, :]) > 0.0:\n",
    "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "            else:\n",
    "                frame_features[i, j, :] = 0.0\n",
    "\n",
    "    return frame_features\n",
    "\n",
    "\n",
    "def predict_action(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"dataset\", path))\n",
    "    frame_features = prepare_single_video(frames)\n",
    "    probabilities = trained_model.predict(frame_features)[0]\n",
    "#     print(f\"Predicted label: {class_vocab[np.argmax(probabilities)]}: {probabilities[np.argmax(probabilities)] * 100:5.2f}\")\n",
    "\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames, class_vocab[np.argmax(probabilities)] \n",
    "\n",
    "\n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "    return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = predict_action(test_video)[0]\n",
    "to_gif(test_frames[:MAX_SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_res(video_file):\n",
    "    frames = predict_action(video_file)\n",
    "    print(video_file,frames[1])\n",
    "    \n",
    "    return frames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predicted'] = test_df['video_name'].apply(check_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myworld",
   "language": "python",
   "name": "myworld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
